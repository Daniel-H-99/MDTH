{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d5be46",
   "metadata": {},
   "source": [
    "# Face Vid2Vid Demo\n",
    "\n",
    "- Paper: One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing (CVPR 2021): [project](https://nvlabs.github.io/face-vid2vid/), [arxiv](https://arxiv.org/abs/2011.15126)\n",
    "- üë©üèª‚Äçüíª Developer : [Jihye Back](https://github.com/happy-jihye)\n",
    "\n",
    "This notebooks is an unofficial demo web app of the `face video2video`.\n",
    "\n",
    "The codes are heavily based on [this code, created by `zhanglonghao1992`](https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis) (thank you!üòä).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f91001",
   "metadata": {},
   "source": [
    "# Gradio App\n",
    "\n",
    "## 1. import libraries & load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d1cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import imageio, cv2\n",
    "from moviepy.editor import *\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "from demo import load_checkpoints, make_animation, find_best_frame\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "\n",
    "config = 'config/vox-256-spade.yaml'\n",
    "checkpoint = 'ckpt/00000189-checkpoint.pth.tar'\n",
    "\n",
    "gen = 'spade'\n",
    "cpu = False\n",
    "\n",
    "generator, kp_detector, he_estimator = load_checkpoints(config_path=config, checkpoint_path=checkpoint, gen=gen, cpu=cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ba0e0",
   "metadata": {},
   "source": [
    "## inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eba94ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(source,\n",
    "              driving,\n",
    "              find_best_frame_ = False,\n",
    "              free_view = False,\n",
    "              yaw = None,\n",
    "              pitch = None,\n",
    "              roll = None,\n",
    "              output_name = 'output.mp4',\n",
    "              \n",
    "              audio = True,\n",
    "              cpu = False,\n",
    "              best_frame = None,\n",
    "              relative = True,\n",
    "              adapt_scale = True,\n",
    "              ):\n",
    "\n",
    "    # source \n",
    "    source_image = resize(source, (256, 256))\n",
    "    \n",
    "    # driving\n",
    "    reader = imageio.get_reader(driving)\n",
    "    fps = reader.get_meta_data()['fps']\n",
    "    driving_video = []\n",
    "    try:\n",
    "        for im in reader:\n",
    "            driving_video.append(im)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    reader.close()\n",
    "\n",
    "    driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n",
    "    \n",
    "    with open(config) as f:\n",
    "        config_ = yaml.load(f)\n",
    "    estimate_jacobian = config_['model_params']['common_params']['estimate_jacobian']\n",
    "    print(f'estimate jacobian: {estimate_jacobian}')\n",
    "\n",
    "    if find_best_frame_ or best_frame is not None:\n",
    "        i = best_frame if best_frame is not None else find_best_frame(source_image, driving_video, cpu=cpu)\n",
    "        print (\"Best frame: \" + str(i))\n",
    "        driving_forward = driving_video[i:]\n",
    "        driving_backward = driving_video[:(i+1)][::-1]\n",
    "        predictions_forward = make_animation(source_image, driving_forward, generator, kp_detector, he_estimator, relative=relative, adapt_movement_scale=adapt_scale, estimate_jacobian=estimate_jacobian, cpu=cpu, free_view=free_view, yaw=yaw, pitch=pitch, roll=roll)\n",
    "        predictions_backward = make_animation(source_image, driving_backward, generator, kp_detector, he_estimator, relative=relative, adapt_movement_scale=adapt_scale, estimate_jacobian=estimate_jacobian, cpu=cpu, free_view=free_view, yaw=yaw, pitch=pitch, roll=roll)\n",
    "        predictions = predictions_backward[::-1] + predictions_forward[1:]\n",
    "    else:\n",
    "        predictions = make_animation(source_image, driving_video, generator, kp_detector, he_estimator, relative=relative, adapt_movement_scale=adapt_scale, estimate_jacobian=estimate_jacobian, cpu=cpu, free_view=free_view, yaw=yaw, pitch=pitch, roll=roll)\n",
    "    \n",
    "    # save video\n",
    "    output_path = 'asset/output'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    print(f'{output_path}/{output_name}')    \n",
    "    \n",
    "    imageio.mimsave(f'{output_path}/{output_name}', [img_as_ubyte(frame) for frame in predictions], fps=fps)\n",
    "    \n",
    "    if audio:\n",
    "        audioclip = VideoFileClip(driving)\n",
    "        audio = audioclip.audio\n",
    "        videoclip = VideoFileClip(f'{output_path}/{output_name}')\n",
    "        videoclip.audio = audio\n",
    "        name = output_name.strip('.mp4')\n",
    "        videoclip.write_videofile(f'{output_path}/{name}_audio.mp4')\n",
    "        return f'./{output_path}/{name}_audio.mp4'\n",
    "    else:\n",
    "        return f'{output_path}/{output_name}'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5b017",
   "metadata": {},
   "source": [
    "## 2. run gradio app ‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64fbc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "samples = []\n",
    "\n",
    "example_source = os.listdir('asset/source')\n",
    "for image in example_source:\n",
    "    samples.append([f'asset/source/{image}', None])\n",
    "\n",
    "example_driving = os.listdir('asset/driving')\n",
    "for video in example_driving:\n",
    "    samples.append([None, f'asset/driving/{video}'])\n",
    "\n",
    "iface = gr.Interface(\n",
    "    inference, # main function\n",
    "    inputs = [ \n",
    "        gr.inputs.Image(shape=(255, 255), label='Source Image'), # source image\n",
    "        gr.inputs.Video(label='Driving Video', type='mp4'), # driving video\n",
    "        \n",
    "        gr.inputs.Checkbox(label=\"fine best frame\", default=False), \n",
    "        gr.inputs.Checkbox(label=\"free view\", default=False), \n",
    "        gr.inputs.Slider(minimum=-90, maximum=90, default=0, step=1, label=\"yaw\"),\n",
    "        gr.inputs.Slider(minimum=-90, maximum=90, default=0, step=1, label=\"pitch\"),\n",
    "        gr.inputs.Slider(minimum=-90, maximum=90, default=0, step=1, label=\"raw\"),\n",
    "        \n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.outputs.Video(label='result') # generated video\n",
    "    ], \n",
    "    \n",
    "    title = 'Face Vid2Vid Demo',\n",
    "    description = \"This app is an unofficial demo web app of the face video2video. The codes are heavily based on this repo, created by zhanglonghao1992\",\n",
    "    \n",
    "    examples = samples,\n",
    "\n",
    "    server_name = '0.0.0.0',\n",
    "    server_port = 8889)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6f58ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:8890/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"http://0.0.0.0:8890/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7faa1804c430>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Flask 'gradio.networking'>, 'http://0.0.0.0:8890/', None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-06 21:17:22,432] ERROR in app: Exception on /file/null [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/flask/app.py\", line 2070, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/flask/app.py\", line 1515, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/flask_cors/extension.py\", line 165, in wrapped_function\n",
      "    return cors_after_request(app.make_response(f(*args, **kwargs)))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/flask/app.py\", line 1513, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/flask/app.py\", line 1499, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/gradio/networking.py\", line 93, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/gradio/networking.py\", line 386, in file\n",
      "    return send_file(os.path.join(app.cwd, path))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/flask/helpers.py\", line 612, in send_file\n",
      "    return werkzeug.utils.send_file(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/werkzeug/utils.py\", line 697, in send_file\n",
      "    stat = os.stat(path)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/workspace/th/face-vid2vid-demo/null'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate jacobian: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:05, 20.90it/s]\n",
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best frame: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:10<00:00,  4.75it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61/61 [00:12<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asset/output/output.mp4\n",
      "Moviepy - Building video asset/output/output_audio.mp4.\n",
      "MoviePy - Writing audio in output_audioTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 0/111 [00:00<?, ?it/s, now=None]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video asset/output/output_audio.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready asset/output/output_audio.mp4\n"
     ]
    }
   ],
   "source": [
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeef1aba",
   "metadata": {},
   "source": [
    "# Inference on notebook\n",
    "\n",
    "If you want to align the raw image, refer to [this repo](https://github.com/happy-jihye/FFHQ-Alignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd764be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "# https://github.com/tg-bomze/Face-Image-Motion-Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "placeholder_bytes = base64.b64decode('iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mP8/x8AAwMCAO+ip1sAAAAASUVORK5CYII=')\n",
    "placeholder_image = imageio.imread(placeholder_bytes, '.png')\n",
    "placeholder_image = resize(placeholder_image, (256, 256))[..., :3]\n",
    "\n",
    "def display(source, driving, generated=None):\n",
    "    fig = plt.figure(figsize=(8 + 4 * (generated is not None), 6))\n",
    "    ims = []\n",
    "    for i in range(len(driving)):\n",
    "        cols = [[placeholder_image], []]\n",
    "        for sourceitem in source:\n",
    "            cols[0].append(sourceitem)\n",
    "        cols[1].append(driving[i])\n",
    "        if generated is not None:\n",
    "            for generateditem in generated:\n",
    "                cols[1].append(generateditem[i])\n",
    "\n",
    "        endcols = []\n",
    "        for thiscol in cols:\n",
    "            endcols.append(np.concatenate(thiscol, axis=1))\n",
    "\n",
    "        im = plt.imshow(np.vstack(endcols), animated=True) # np.concatenate(cols[0], axis=1)\n",
    "        plt.axis('off')\n",
    "        ims.append([im])\n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)\n",
    "    plt.close()\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b318c563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "265it [00:12, 21.33it/s]\n",
      "  0%|          | 0/57 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best frame: 208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:10<00:00,  5.30it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 209/209 [00:39<00:00,  5.32it/s]\n",
      "265it [00:13, 19.72it/s]\n",
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best frame: 214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:09<00:00,  5.31it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 215/215 [00:40<00:00,  5.30it/s]\n",
      "265it [00:12, 21.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best frame: 186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:15<00:00,  5.26it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187/187 [00:35<00:00,  5.32it/s]\n"
     ]
    }
   ],
   "source": [
    "source_list = ['asset/source/3.png', 'asset/source/5.png', 'asset/source/4.png']\n",
    "driving = 'asset/driving/1.mp4'\n",
    "\n",
    "# saving path\n",
    "os.makedirs('asset/output', exist_ok=True)\n",
    "\n",
    "find_best_frame_ = True\n",
    "free_view = False\n",
    "yaw = None\n",
    "pitch = None\n",
    "roll = None\n",
    "\n",
    "cpu = False\n",
    "best_frame = None\n",
    "relative = True\n",
    "adapt_scale = True\n",
    "estimate_jacobian = False\n",
    "\n",
    "\n",
    "# driving\n",
    "reader = imageio.get_reader(driving)\n",
    "fps = reader.get_meta_data()['fps']\n",
    "driving_video = []\n",
    "try:\n",
    "    for im in reader:\n",
    "        driving_video.append(im)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "reader.close()\n",
    "driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n",
    "\n",
    "source_images = []\n",
    "final = []\n",
    "for idx, source in enumerate(source_list):\n",
    "    # source \n",
    "    source_image = imageio.imread(source)[..., :3]\n",
    "    source_image = resize(source_image, (256, 256))\n",
    "    source_images.append(source_image)\n",
    "    \n",
    "    # inference\n",
    "    if find_best_frame_ or best_frame is not None:\n",
    "        i = best_frame if best_frame is not None else find_best_frame(source_image, driving_video, cpu=cpu)\n",
    "        print (\"Best frame: \" + str(i))\n",
    "        driving_forward = driving_video[i:]\n",
    "        driving_backward = driving_video[:(i+1)][::-1]\n",
    "        predictions_forward = make_animation(source_image, driving_forward, generator, kp_detector, he_estimator, relative=relative, adapt_movement_scale=adapt_scale, estimate_jacobian=estimate_jacobian, cpu=cpu, free_view=free_view, yaw=yaw, pitch=pitch, roll=roll)\n",
    "        predictions_backward = make_animation(source_image, driving_backward, generator, kp_detector, he_estimator, relative=relative, adapt_movement_scale=adapt_scale, estimate_jacobian=estimate_jacobian, cpu=cpu, free_view=free_view, yaw=yaw, pitch=pitch, roll=roll)\n",
    "        predictions = predictions_backward[::-1] + predictions_forward[1:]\n",
    "    else:\n",
    "        predictions = make_animation(source_image, driving_video, generator, kp_detector, he_estimator, relative=relative, adapt_movement_scale=adapt_scale, estimate_jacobian=estimate_jacobian, cpu=cpu, free_view=free_view, yaw=yaw, pitch=pitch, roll=roll)\n",
    "\n",
    "    imageio.mimsave(f'asset/output/{idx}.mp4', [img_as_ubyte(frame) for frame in predictions])\n",
    "    final.append(predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b30edabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_video = display(source_images, driving_video, final)\n",
    "final_video.save(f'asset/output/final_1.mp4', fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fe7c4644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"asset/output/final_1.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video('asset/output/final_1.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447fd404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
